{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pytest\n",
    "import ipytest\n",
    "ipytest.autoconfig()  # Configurar ipytest para usar pytest en Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el dataset desde un archivo CSV\n",
    "dataset = pd.read_csv(r\"../../data/processed/TCGA_GBM_LGG_Mutations_clean_v2.csv\") # cambiar la ruta al probar\n",
    "\n",
    "# Crear un DataFrame a partir del dataset leído\n",
    "dataset_df = pd.DataFrame(dataset, columns=dataset.columns)\n",
    "\n",
    "# Convertir la columna \"Grade\" a tipo categórico\n",
    "dataset_df[\"Grade\"] = dataset_df[\"Grade\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePipeline:\n",
    "    def __init__(self):\n",
    "        self.frame = None\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = None, None, None, None\n",
    "        self.model = None\n",
    "        self.load_dataset()\n",
    "    \n",
    "    def load_dataset(self):\n",
    "        \"\"\"Carga el dataset y realiza la división en conjuntos de entrenamiento y prueba.\"\"\"\n",
    "        # Leer el dataset desde un archivo CSV\n",
    "        dataset = pd.read_csv(r\"../../data/processed/TCGA_GBM_LGG_Mutations_clean_v2.csv\") # cambiar la ruta al probar\n",
    "        \n",
    "        # Crear un DataFrame a partir del dataset leído\n",
    "        self.frame = pd.DataFrame(dataset, columns=dataset.columns)\n",
    "        \n",
    "        # Convertir la columna \"Grade\" a tipo categórico\n",
    "        self.frame[\"Grade\"] = self.frame[\"Grade\"].astype('category')\n",
    "        \n",
    "        # Obtener los nombres de las características excluyendo la columna 'Grade'\n",
    "        feature_names = [col for col in self.frame.columns if col != 'Grade']\n",
    "        \n",
    "        # Dividir el dataset en conjuntos de entrenamiento y prueba\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.frame[feature_names], self.frame['Grade'], test_size=0.65, random_state=42)\n",
    "        \n",
    "    def train(self, algorithm=LogisticRegression):\n",
    "        \"\"\"Entrenar el modelo usando el algoritmo especificado (por defecto LogisticRegression).\"\"\"\n",
    "        # Inicializar el modelo con el algoritmo especificado\n",
    "        self.model = algorithm(solver='lbfgs', multi_class='auto')\n",
    "        \n",
    "        # Entrenar el modelo con los datos de entrenamiento\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "    def predict(self, input_data):\n",
    "        \"\"\"Realizar predicciones usando el modelo entrenado.\"\"\"\n",
    "        # Predecir los resultados para los datos de entrada\n",
    "        return self.model.predict(input_data)\n",
    "        \n",
    "    def get_accuracy(self):\n",
    "        \"\"\"Obtener la precisión del modelo en el conjunto de prueba.\"\"\"\n",
    "        # Calcular y devolver la precisión del modelo en los datos de prueba\n",
    "        return self.model.score(X=self.X_test, y=self.y_test)\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        \"\"\"Método de ejecución para correr el pipeline varias veces.\"\"\"\n",
    "        # Cargar el dataset y entrenar el modelo\n",
    "        self.load_dataset()\n",
    "        self.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineWithFeatureEngineering(SimplePipeline):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(self.X_train)\n",
    "    \n",
    "    def apply_scaler(self):\n",
    "        # Aplica la normalización a los conjuntos de entrenamiento y prueba\n",
    "        self.X_train = self.scaler.transform(self.X_train)\n",
    "        self.X_test = self.scaler.transform(self.X_test)\n",
    "        \n",
    "    def predict(self, input_data):\n",
    "        # Normaliza los datos de entrada antes de realizar predicciones\n",
    "        scaled_input_data = self.scaler.transform(input_data)\n",
    "        return self.model.predict(scaled_input_data)\n",
    "                  \n",
    "    def run_pipeline(self):\n",
    "        # Carga el dataset, aplica la normalización y entrena el modelo\n",
    "        self.load_dataset()\n",
    "        self.apply_scaler()\n",
    "        self.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Pruebas\n",
    "\n",
    "Realizaremos dos pruebas diferentes:\n",
    "\n",
    "    Prueba de referencia: Comparar la precisión del modelo contra un benchmark simple\n",
    "    Prueba diferencial: Comparar la precisión de un modelo contra el otro\n",
    "\n",
    "Primero, predeciremos la clase más común.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Grade\n",
       "0    497\n",
       "1    360\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cuenta la cantidad de ocurrencias de cada valor en la columna \"Grade\" del DataFrame `dataset_df`.\n",
    "\n",
    "Returns:\n",
    "    pandas.Series: Una serie que contiene el conteo de cada valor único en la columna \"Grade\".\n",
    "\"\"\"\n",
    "\n",
    "dataset_df[\"Grade\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def pipelines():\n",
    "    # Crear una instancia de la primera versión del pipeline\n",
    "    pipeline_v1 = SimplePipeline()\n",
    "    \n",
    "    # Crear una instancia de la segunda versión del pipeline con ingeniería de características\n",
    "    pipeline_v2 = PipelineWithFeatureEngineering()\n",
    "    \n",
    "    # Ejecutar el pipeline de la primera versión\n",
    "    pipeline_v1.run_pipeline()\n",
    "    \n",
    "    # Ejecutar el pipeline de la segunda versión\n",
    "    pipeline_v2.run_pipeline()\n",
    "    \n",
    "    # Devolver ambas instancias de los pipelines\n",
    "    return pipeline_v1, pipeline_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.08s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_accuracy_higher_than_benchmark(pipelines):\n",
    "    pipeline_v1, _ = pipelines\n",
    "    \n",
    "    # Benchmark inicial: Predicciones de la clase más común\n",
    "    benchmark_predictions = [1.0] * len(ppipeline_v1.y_test)\n",
    "    benchmark_accuracy = accuracy_score(y_true=pipeline_v1.y_test, y_pred=benchmark_predictions)\n",
    "    \n",
    "    # Obtener la precisión del modelo\n",
    "    predictions = pipeline_v1.predict(pipeline_v1.X_test)\n",
    "    actual_accuracy = accuracy_score(y_true=pipeline_v1.y_test, y_pred=predictions)\n",
    "    \n",
    "    # Imprimir la precisión del modelo y del benchmark\n",
    "    print(f'Accuracy of model 1: {actual_accuracy}, Accuracy of Benchmark: {benchmark_accuracy}')\n",
    "    \n",
    "    # Comparar la precisión del primer modelo contra el benchmark\n",
    "    assert actual_accuracy > benchmark_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.06s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_accuracy_compared_to_previous_version(pipelines):\n",
    "    \"\"\"\n",
    "    Compara la precisión de dos versiones de un pipeline de modelos.\n",
    "    Args:\n",
    "        pipelines (tuple): Una tupla que contiene dos pipelines de modelos, \n",
    "                           donde el primer elemento es la versión 1 y el segundo \n",
    "                           elemento es la versión 2.\n",
    "    Raises:\n",
    "        AssertionError: Si la precisión de la versión 2 es menor que la precisión \n",
    "                        de la versión 1.\n",
    "    Comentarios:\n",
    "        - Obtiene la precisión de cada versión del pipeline.\n",
    "        - Imprime la precisión de cada modelo.\n",
    "        - Compara la precisión del segundo modelo contra el primero.\n",
    "    \"\"\"\n",
    "    pipeline_v1, pipeline_v2 = pipelines\n",
    "    \n",
    "    # Obtener la precisión de cada versión\n",
    "    v1_accuracy = pipeline_v1.get_accuracy()\n",
    "    v2_accuracy = pipeline_v2.get_accuracy()\n",
    "    \n",
    "    # Imprimir la precisión de cada modelo\n",
    "    print(f'Accuracy of model 1: {v1_accuracy}')\n",
    "    print(f'Accuracy of model 2: {v2_accuracy}')\n",
    "    \n",
    "    # Comparar la precisión del segundo modelo contra el primero\n",
    "    assert v2_accuracy >= v1_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
